{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e9b2253defb52026888435a3b51036293fbdc37"
      },
      "cell_type": "code",
      "source": "!pip install pretty-midi\nimport sys\nsys.path.append('../')\n\nimport pretty_midi\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch import utils\nfrom torchvision import datasets, transforms\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom copy import deepcopy\nimport random\n%matplotlib inline",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting pretty-midi\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/82/ee67696b85ca3be267c67a46595545e719eec677dcd94e3cf827db833fb8/pretty_midi-0.2.8.tar.gz (5.6MB)\n\u001b[K    100% |████████████████████████████████| 5.6MB 7.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from pretty-midi) (1.16.2)\nCollecting mido>=1.1.16 (from pretty-midi)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n\u001b[K    100% |████████████████████████████████| 61kB 19.5MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from pretty-midi) (1.12.0)\nBuilding wheels for collected packages: pretty-midi\n  Building wheel for pretty-midi (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/22/e7/6b/70eb5879f7dbcb4f44fee735a61d6298f9e082be8538b52422\nSuccessfully built pretty-midi\nInstalling collected packages: mido, pretty-midi\nSuccessfully installed mido-1.2.9 pretty-midi-0.2.8\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb0a7d51f30b1e0a2b7cdb2f934e80e630e320be"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f2fcdfa988834ab9ed6aa447079134beb86ded4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4ce5250a110185956cfcd8655d0fe0bb769f852"
      },
      "cell_type": "code",
      "source": "#for msg in mid.play():\n#    print(msg)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "4799a0b077b1fab9f44bf0f093af776e2d307150"
      },
      "cell_type": "code",
      "source": "#for msg in mid.play():\n\n   # print(msg.type, msg.note, msg.velocity, msg.time)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "279342afd03173326bca1d26c5a9faa28dfc412e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "758db98ac99f636cd84a8d0e843926c6cee5ef19"
      },
      "cell_type": "code",
      "source": "import os, argparse, time\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\nfrom keras.optimizers import SGD, Adam\nfrom keras.models import model_from_json\nfrom multiprocessing import Pool as ThreadPool",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d13a683dd9b1f0bed1fc6adfc7b45434dcea890"
      },
      "cell_type": "code",
      "source": "def get_data_generator(midi_paths, \n                       window_size=40, \n                       batch_size=32,\n                       num_threads=8,\n                       max_files_in_ram=170):\n\n    if num_threads > 1:\n        # load midi data\n        pool = ThreadPool(num_threads)\n\n    load_index = 0\n\n    while True:\n        load_files = midi_paths[load_index:load_index + max_files_in_ram]\n        # print('length of load files: {}'.format(len(load_files)))\n        load_index = (load_index + max_files_in_ram) % len(midi_paths)\n\n        # print('loading large batch: {}'.format(max_files_in_ram))\n        # print('Parsing midi files...')\n        # start_time = time.time()\n        if num_threads > 1:\n            parsed = pool.map(parse_midi, load_files)\n        else:\n            parsed = map(parse_midi, load_files)\n        # print('Finished in {:.2f} seconds'.format(time.time() - start_time))\n        # print('parsed, now extracting data')\n\n        data = _windows_from_monophonic_instruments(parsed, window_size)\n\n        batch_index = 0\n        while batch_index + batch_size < len(data[0]):\n            # print('getting data...')\n            # print('yielding small batch: {}'.format(batch_size))\n            \n            res = (data[0][batch_index: batch_index + batch_size], \n                   data[1][batch_index: batch_index + batch_size])\n\n            yield res\n            batch_index = batch_index + batch_size\n",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e3d869e17ecf2e3d64886ceb2293ffe04587b76"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d5f512ae69e258e292b799cbfd520a14f49ea41"
      },
      "cell_type": "code",
      "source": "OUTPUT_SIZE = 129 # 0-127 notes + 1 for rests\n\n\n",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74fd62cfc12b63f824f49a37827c5f674fd3d061",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9d8ea7918a9e7f995de1ca85e2ded560d96737d"
      },
      "cell_type": "code",
      "source": "def parse_midi(path):\n    midi = None\n    try:\n        midi = pretty_midi.PrettyMIDI(path)\n        midi.remove_invalid_notes()\n    except Exception as e:\n        raise Exception((\"%s\\nerror readying midi file %s\" % (e, path)))\n    return midi\n\n\ndef log(message, verbose):\n\tif verbose:\n\t\tprint('[*] {}'.format(message))\n\ndef create_experiment_dir(experiment_dir, verbose=False):\n    \n    # if the experiment directory was specified and already exists\n    if experiment_dir != 'experiments/default' and \\\n       os.path.exists(experiment_dir):\n    \t# raise an error\n    \traise Exception('Error: Invalid --experiment_dir, {} already exists' \\\n    \t\t            .format(experiment_dir))\n\n    # if the experiment directory was not specified, create a new numeric folder\n    if experiment_dir == 'experiments/default':\n    \t\n    \texperiments = os.listdir('experiments')\n    \texperiments = [dir_ for dir_ in experiments \\\n    \t               if os.path.isdir(os.path.join('experiments', dir_))]\n    \t\n    \tmost_recent_exp = 0\n    \tfor dir_ in experiments:\n    \t\ttry:\n    \t\t\tmost_recent_exp = max(int(dir_), most_recent_exp)\n    \t\texcept ValueError as e:\n    \t\t\t# ignrore non-numeric folders in experiments/\n    \t\t\tpass\n\n    \texperiment_dir = os.path.join('experiments', \n    \t\t                          str(most_recent_exp + 1).rjust(2, '0'))\n\n    os.mkdir(experiment_dir)\n    log('Created experiment directory {}'.format(experiment_dir), verbose)\n    os.mkdir(os.path.join(experiment_dir, 'checkpoints'))\n    log('Created checkpoint directory {}'.format(os.path.join(experiment_dir, 'checkpoints')),\n    \tverbose)\n    os.mkdir(os.path.join(experiment_dir, 'tensorboard-logs'))\n    log('Created log directory {}'.format(os.path.join(experiment_dir, 'tensorboard-logs')), \n    \tverbose)\n\n    return experiment_dir",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "81733f90ccf4b35b8dd968258104731a8050320b"
      },
      "cell_type": "code",
      "source": "def get_callbacks(experiment_dir, checkpoint_monitor='val_acc'):\n    \n    callbacks = []\n    \n    # save model checkpoints\n    filepath = os.path.join(experiment_dir, \n                            'checkpoints', \n                            'checkpoint-epoch_{epoch:03d}-val_acc_{val_acc:.3f}.hdf5')\n\n    callbacks.append(ModelCheckpoint(filepath, \n                                     monitor=checkpoint_monitor, \n                                     verbose=1, \n                                     save_best_only=False, \n                                     mode='max'))\n\n    callbacks.append(ReduceLROnPlateau(monitor='val_loss', \n                                       factor=0.5, \n                                       patience=3, \n                                       verbose=1, \n                                       mode='auto', \n                                       epsilon=0.0001, \n                                       cooldown=0, \n                                       min_lr=0))\n\n    callbacks.append(TensorBoard(log_dir=os.path.join(experiment_dir, 'tensorboard-logs'), \n                                histogram_freq=0, \n                                write_graph=True, \n                                write_images=False))\n\n    return callbacks",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d79173f97f60b8f1d36fa7e16b513052a81af386"
      },
      "cell_type": "code",
      "source": "def _windows_from_monophonic_instruments(midi, window_size):\n    X, y = [], []\n    for m in midi:\n        if m is not None:\n            melody_instruments = filter_monophonic(m.instruments, 1.0)\n            for instrument in melody_instruments:\n                if len(instrument.notes) > window_size:\n                    windows = _encode_sliding_windows(instrument, window_size)\n                    for w in windows:\n                        X.append(w[0])\n                        y.append(w[1])\n    return (np.asarray(X), np.asarray(y))\n\n# one-hot encode a sliding window of notes from a pretty midi instrument.\n# This approach uses the piano roll method, where each step in the sliding\n# window represents a constant unit of time (fs=4, or 1 sec / 4 = 250ms).\n# This allows us to encode rests.\n# expects pm_instrument to be monophonic.\ndef _encode_sliding_windows(pm_instrument, window_size):\n    \n    roll = np.copy(pm_instrument.get_piano_roll(fs=6).T)\n\n    # trim beginning silence\n    summed = np.sum(roll, axis=1)\n    mask = (summed > 0).astype(float)\n    roll = roll[np.argmax(mask):]\n    \n    # transform note velocities into 1s\n    roll = (roll > 0).astype(float)\n    \n    # calculate the percentage of the events that are rests\n    # s = np.sum(roll, axis=1)\n    # num_silence = len(np.where(s == 0)[0])\n    # print('{}/{} {:.2f} events are rests'.format(num_silence, len(roll), float(num_silence)/float(len(roll))))\n\n    # append a feature: 1 to rests and 0 to notes\n    rests = np.sum(roll, axis=1)\n    rests = (rests != 1).astype(float)\n    roll = np.insert(roll, 0, rests, axis=1)\n    \n    windows = []\n    for i in range(0, roll.shape[0] - window_size - 1):\n        windows.append((roll[i:i + window_size], roll[i + window_size + 1]))\n    return windows",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "244c63d6718159f8310956c4f8895abe2cee6c4e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf7a4e5f4519987890184d370e31de9079c511ee"
      },
      "cell_type": "code",
      "source": "def get_percent_monophonic(pm_instrument_roll):\n    mask = pm_instrument_roll.T > 0\n    notes = np.sum(mask, axis=1)\n    n = np.count_nonzero(notes)\n    single = np.count_nonzero(notes == 1)\n    if single > 0:\n        return float(single) / float(n)\n    elif single == 0 and n > 0:\n        return 0.0\n    else: # no notes of any kind\n        return 0.0\n    \ndef filter_monophonic(pm_instruments, percent_monophonic=0.99):\n    return [i for i in pm_instruments if \\\n            get_percent_monophonic(i.get_piano_roll()) >= percent_monophonic]",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a39ea9ede984b6a3cc45ac5ac5953486cd3a4883"
      },
      "cell_type": "code",
      "source": "def generate(model, seeds, window_size, length, num_to_gen, instrument_name):\n    \n    # generate a pretty midi file from a model using a seed\n    def _gen(model, seed, window_size, length):\n        \n        generated = []\n        # ring buffer\n        buf = np.copy(seed).tolist()\n        while len(generated) < length:\n            arr = np.expand_dims(np.asarray(buf), 0)\n            pred = model.predict(arr)\n            \n            # argmax sampling (NOT RECOMMENDED), or...\n            # index = np.argmax(pred)\n            \n            # prob distrobuition sampling\n            index = np.random.choice(range(0, seed.shape[1]), p=pred[0])\n            pred = np.zeros(seed.shape[1])\n\n            pred[index] = 1\n            generated.append(pred)\n            buf.pop(0)\n            buf.append(pred)\n\n        return generated\n\n    midis = []\n    for i in range(0, num_to_gen):\n        seed = seeds[random.randint(0, len(seeds) - 1)]\n        gen = _gen(model, seed, window_size, length)\n        midis.append(_network_output_to_midi(gen, instrument_name))\n    return midis\n\n# create a pretty midi file with a single instrument using the one-hot encoding\n# output of keras model.predict.\ndef _network_output_to_midi(windows, \n                           instrument_name='Acoustic Grand Piano', \n                           allow_represses=False):\n\n    # Create a PrettyMIDI object\n    midi = pretty_midi.PrettyMIDI()\n    # Create an Instrument instance for a cello instrument\n    instrument_program = pretty_midi.instrument_name_to_program(instrument_name)\n    instrument = pretty_midi.Instrument(program=instrument_program)\n    \n    cur_note = None # an invalid note to start with\n    cur_note_start = None\n    clock = 0\n\n    # Iterate over note names, which will be converted to note number later\n    for step in windows:\n\n        note_num = np.argmax(step) - 1\n        \n        # a note has changed\n        if allow_represses or note_num != cur_note:\n            \n            # if a note has been played before and it wasn't a rest\n            if cur_note is not None and cur_note >= 0:            \n                # add the last note, now that we have its end time\n                note = pretty_midi.Note(velocity=127, \n                                        pitch=int(cur_note), \n                                        start=cur_note_start, \n                                        end=clock)\n                instrument.notes.append(note)\n\n            # update the current note\n            cur_note = note_num\n            cur_note_start = clock\n\n        # update the clock\n        clock = clock + 1.0 / 4\n\n    # Add the cello instrument to the PrettyMIDI object\n    midi.instruments.append(instrument)\n    return midi\n",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8adfec5c2630153e9843b8335772c321d1001824"
      },
      "cell_type": "code",
      "source": "keras.backend.clear_session()",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af83356e74a88ea8891685f632f0764a17a3d102"
      },
      "cell_type": "code",
      "source": "\n\noptimizer = Adam\nepoch = 0\nnum_layers = 4\nmodel = Sequential()\nfor layer_index in range(num_layers):\n    kwargs = dict() \n    kwargs['units'] = 64\n    # if this is the first layer\n    if layer_index == 0:\n        kwargs['input_shape'] = (40, OUTPUT_SIZE)\n        if num_layers == 1:\n            kwargs['return_sequences'] = False\n        else:\n            kwargs['return_sequences'] = True\n        model.add(LSTM(**kwargs))\n    else:\n        # if this is a middle layer\n        if not layer_index == num_layers - 1:\n            kwargs['return_sequences'] = True\n            model.add(LSTM(**kwargs))\n        else: # this is the last layer\n            kwargs['return_sequences'] = False\n            model.add(LSTM(**kwargs))\n    model.add(Dropout(0.2))\n    \nmodel.add(Dense(OUTPUT_SIZE))\nmodel.add(Activation('softmax'))\n\n\noptimizer = Adam()\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=optimizer,\n              metrics=['accuracy'])\n",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ec4d9227d7dde552015ebacbf97f1088f77cc75"
      },
      "cell_type": "code",
      "source": "\n\n\n\nmidi_files = [os.path.join(\"../input/music1\", path) \n                  for path in os.listdir(\"../input/music1\") \\\n                  if '.mid' in path or '.midi' in path]\n\nprint(len(midi_files))\n\nexperiment_dir = create_experiment_dir('experiment_dir4', 1)\n\nval_split = 0.2 # use 20 percent for validation\nval_split_index = int(float(len(midi_files)) * val_split)\n\n# use generators to lazy load train/validation data, ensuring that the\n# user doesn't have to load all midi files into RAM at once\ntrain_generator = get_data_generator(midi_files[0:val_split_index])\n\nval_generator = get_data_generator(midi_files[val_split_index:])\n\ncallbacks = get_callbacks(experiment_dir)\nprint('fitting model...')\n# this is a somewhat magic number which is the average number of length-20 windows\n# calculated from ~5K MIDI files from the Lakh MIDI Dataset.\nmagic_number = 827\nbatch_size = 60\nstart_time = time.time()\nnum_epochs = 20\n\nmodel.fit_generator(train_generator,\n                    steps_per_epoch=len(midi_files) * magic_number / batch_size, \n                    epochs=num_epochs,\n                    validation_data=val_generator, \n                    validation_steps=len(midi_files) * 0.2 * magic_number / batch_size,\n                    verbose=1, \n                    callbacks=callbacks,\n                    initial_epoch=0)\nlog('Finished in {:.2f} seconds'.format(time.time() - start_time), 1)\n\n\n\n",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "5\n[*] Created experiment directory experiment_dir4\n[*] Created checkpoint directory experiment_dir4/checkpoints\n[*] Created log directory experiment_dir4/tensorboard-logs\nfitting model...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n  warnings.warn('`epsilon` argument is deprecated and '\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Epoch 1/20\n69/68 [==============================] - 28s 403ms/step - loss: 3.4107 - acc: 0.1309 - val_loss: 5.8494 - val_acc: 0.0000e+00\n\nEpoch 00001: saving model to experiment_dir4/checkpoints/checkpoint-epoch_001-val_acc_0.000.hdf5\nEpoch 2/20\n69/68 [==============================] - 23s 338ms/step - loss: 2.5484 - acc: 0.1513 - val_loss: 7.0003 - val_acc: 0.0000e+00\n\nEpoch 00002: saving model to experiment_dir4/checkpoints/checkpoint-epoch_002-val_acc_0.000.hdf5\nEpoch 3/20\n69/68 [==============================] - 24s 346ms/step - loss: 2.4928 - acc: 0.1626 - val_loss: 7.4786 - val_acc: 0.0089\n\nEpoch 00003: saving model to experiment_dir4/checkpoints/checkpoint-epoch_003-val_acc_0.009.hdf5\nEpoch 4/20\n69/68 [==============================] - 24s 343ms/step - loss: 2.1434 - acc: 0.2767 - val_loss: 7.9988 - val_acc: 0.0246\n\nEpoch 00004: saving model to experiment_dir4/checkpoints/checkpoint-epoch_004-val_acc_0.025.hdf5\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 5/20\n69/68 [==============================] - 23s 336ms/step - loss: 1.9559 - acc: 0.2871 - val_loss: 6.5796 - val_acc: 0.0223\n\nEpoch 00005: saving model to experiment_dir4/checkpoints/checkpoint-epoch_005-val_acc_0.022.hdf5\nEpoch 6/20\n69/68 [==============================] - 23s 338ms/step - loss: 1.9312 - acc: 0.2817 - val_loss: 7.3067 - val_acc: 0.0536\n\nEpoch 00006: saving model to experiment_dir4/checkpoints/checkpoint-epoch_006-val_acc_0.054.hdf5\nEpoch 7/20\n69/68 [==============================] - 24s 343ms/step - loss: 1.8975 - acc: 0.2889 - val_loss: 7.6572 - val_acc: 0.0714\n\nEpoch 00007: saving model to experiment_dir4/checkpoints/checkpoint-epoch_007-val_acc_0.071.hdf5\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 8/20\n69/68 [==============================] - 24s 344ms/step - loss: 1.8491 - acc: 0.3170 - val_loss: 8.2624 - val_acc: 0.0000e+00\n\nEpoch 00008: saving model to experiment_dir4/checkpoints/checkpoint-epoch_008-val_acc_0.000.hdf5\nEpoch 9/20\n69/68 [==============================] - 24s 341ms/step - loss: 1.8223 - acc: 0.3311 - val_loss: 7.0434 - val_acc: 0.0067\n\nEpoch 00009: saving model to experiment_dir4/checkpoints/checkpoint-epoch_009-val_acc_0.007.hdf5\nEpoch 10/20\n69/68 [==============================] - 24s 343ms/step - loss: 1.7466 - acc: 0.3818 - val_loss: 7.9666 - val_acc: 0.0000e+00\n\nEpoch 00010: saving model to experiment_dir4/checkpoints/checkpoint-epoch_010-val_acc_0.000.hdf5\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 11/20\n69/68 [==============================] - 23s 340ms/step - loss: 1.6860 - acc: 0.3822 - val_loss: 8.3965 - val_acc: 0.0022\n\nEpoch 00011: saving model to experiment_dir4/checkpoints/checkpoint-epoch_011-val_acc_0.002.hdf5\nEpoch 12/20\n69/68 [==============================] - 24s 344ms/step - loss: 1.6664 - acc: 0.3890 - val_loss: 9.0007 - val_acc: 0.0000e+00\n\nEpoch 00012: saving model to experiment_dir4/checkpoints/checkpoint-epoch_012-val_acc_0.000.hdf5\nEpoch 13/20\n69/68 [==============================] - 24s 351ms/step - loss: 1.6555 - acc: 0.3822 - val_loss: 7.4602 - val_acc: 0.0045\n\nEpoch 00013: saving model to experiment_dir4/checkpoints/checkpoint-epoch_013-val_acc_0.004.hdf5\n\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 14/20\n69/68 [==============================] - 25s 357ms/step - loss: 1.6306 - acc: 0.3795 - val_loss: 8.3840 - val_acc: 0.0022\n\nEpoch 00014: saving model to experiment_dir4/checkpoints/checkpoint-epoch_014-val_acc_0.002.hdf5\nEpoch 15/20\n69/68 [==============================] - 24s 341ms/step - loss: 1.6071 - acc: 0.3759 - val_loss: 8.7874 - val_acc: 0.0000e+00\n\nEpoch 00015: saving model to experiment_dir4/checkpoints/checkpoint-epoch_015-val_acc_0.000.hdf5\nEpoch 16/20\n69/68 [==============================] - 24s 346ms/step - loss: 1.6065 - acc: 0.3990 - val_loss: 9.2956 - val_acc: 0.0000e+00\n\nEpoch 00016: saving model to experiment_dir4/checkpoints/checkpoint-epoch_016-val_acc_0.000.hdf5\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 17/20\n69/68 [==============================] - 23s 339ms/step - loss: 1.5918 - acc: 0.3854 - val_loss: 7.5757 - val_acc: 0.0022\n\nEpoch 00017: saving model to experiment_dir4/checkpoints/checkpoint-epoch_017-val_acc_0.002.hdf5\nEpoch 18/20\n69/68 [==============================] - 24s 343ms/step - loss: 1.5728 - acc: 0.4008 - val_loss: 8.5430 - val_acc: 0.0022\n\nEpoch 00018: saving model to experiment_dir4/checkpoints/checkpoint-epoch_018-val_acc_0.002.hdf5\nEpoch 19/20\n69/68 [==============================] - 24s 344ms/step - loss: 1.5863 - acc: 0.4049 - val_loss: 8.9395 - val_acc: 0.0000e+00\n\nEpoch 00019: saving model to experiment_dir4/checkpoints/checkpoint-epoch_019-val_acc_0.000.hdf5\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\nEpoch 20/20\n69/68 [==============================] - 24s 346ms/step - loss: 1.5776 - acc: 0.4072 - val_loss: 9.3884 - val_acc: 0.0000e+00\n\nEpoch 00020: saving model to experiment_dir4/checkpoints/checkpoint-epoch_020-val_acc_0.000.hdf5\n[*] Finished in 485.65 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50a8e78a9f763e3f515bb977c433a4580e610a20"
      },
      "cell_type": "code",
      "source": "# generate 10 tracks using random seeds\nlog('Loading seed files...', 1)\nseed_generator = get_data_generator(midi_files, \n                                              window_size=40,\n                                              batch_size=32,\n                                              num_threads=1,\n                                              max_files_in_ram=10)\n\n\nX, y = next(seed_generator)\ngenerated = generate(model, X, 40, \n                      100, 10, 'Acoustic Grand Piano')\nif not os.path.isdir('output'):\n    os.makedirs('output')\n\nfor i, midi in enumerate(generated):\n    file = os.path.join('output', '{}.mid'.format(i + 1))\n    midi.write(file.format(i + 1))\n    log('wrote midi file to {}'.format(file), True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97b8c62b4dee72ddff5f10c905605b07c10074a5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c172a5f75cb2db5dd6a4fc30b8bfc163cab3d98"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e01a7d4e6f18c7c4637bd2cbdda879bf09ea28b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b0bd158a3a8b4543434215ed066695d3dca0774"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}