{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 – Convolutional neural network\n",
    "\n",
    "### Materials\n",
    "1. [ImageNet](http://www.image-net.org)\n",
    "2. [Overview](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) on wiki.\n",
    "3. Stanford's [course](http://cs231n.stanford.edu) on convolutional networks + some [materials](http://cs231n.github.io/convolutional-networks/) on github.\n",
    "4. [Pooling](https://arxiv.org/pdf/1412.6806.pdf)\n",
    "5. [Dropout](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "6. [Batch normalization](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "7. [Data augmentation](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    "\n",
    "### Models\n",
    "1. [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "2. [AlexNet](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012)\n",
    "3. [VGGNet](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "4. [GoogLeNet](https://arxiv.org/pdf/1409.4842)\n",
    "5. [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "### Tutorials\n",
    "1. [Guide](https://www.tensorflow.org/tutorials/layers) to conv nets training\n",
    "2. More advanced [tutorial](https://www.tensorflow.org/tutorials/deep_cnn)\n",
    "3. How to [using GPUs](https://www.tensorflow.org/tutorials/using_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolution\n",
    "Convolutional Layer is the core building block of a conv nets.\n",
    "\n",
    "#### Overview and intuition\n",
    "The conv layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of some conv net might have size $3 \\times 3 \\times 3$ (i.e. 3 pixels width and height, and 3 because images have depth 3, RGB channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each conv layer, and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.\n",
    "\n",
    "#### Local connectivity\n",
    "When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in space (along width and height), but always full along the entire depth of the input volume.\n",
    "\n",
    "#### Spatial arrangement.\n",
    "We have explained the connectivity of each neuron in the conv layer to the input volume, but we have not yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding.\n",
    "\n",
    "1. First, the **depth** of the output volume. It corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column.\n",
    "\n",
    "2. Second, we must specify the **stride** with which we slide the filter. When the stride is $1$ then we move the filters one pixel at a time. When the stride is $2$ (or uncommonly $3$ or more, though this is rare in practice) then the filters jump $2$ pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n",
    "\n",
    "3. As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n",
    "\n",
    "#### Implementation as matrix multiplication\n",
    "Note that the convolution operation essentially performs dot products between the filters and local regions of the input. A common implementation pattern of the conv layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply. This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated multiple times. However, the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used [BLAS API](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)).\n",
    "\n",
    "#### Backpropagation\n",
    "The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters). This is easy to derive in the 1-dimensional case with a toy example.\n",
    "\n",
    "#### Convolution $1 \\times 1$\n",
    "Several papers use $1 \\times 1$ convolutions. Some people are at first confused to see $1 \\times 1$ convolutions especially when they come from signal processing background. Normally signals are 2-dimensional so $1 \\times 1$ convolutions do not make sense (it’s just pointwise scaling). However, we must remember that we operate over 3-dimensional volumes, and that the filters always extend through the full depth of the input volume. For example, if the input is $n \\times n \\times 3$ then doing $1 \\times 1$ convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classic architecture\n",
    "The crown of straightforward architectures for convolutional networks is probably [VGG](https://arxiv.org/pdf/1409.1556.pdf). In fact it is a chain of a fixed set of layers. The most common form of a conv net architecture stacks a few conv + ReLu layers, follows them with pool layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully connected layers. The last fully connected layer holds the output, such as the class scores. In more detail below.\n",
    "\n",
    "#### Convolution layer\n",
    "The conv layers should be using small filters (e.g. $3 \\times 3$ or at most $5 \\times 5$), using a stride equals $1$, and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when filter size is $3$ then using padding equals $1$ to preserves the input size.\n",
    "\n",
    "#### Pooling layer\n",
    "Another important concept is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which max pooling is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. The intuition is that the exact location of a feature is less important than its rough location relative to other features. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n",
    "\n",
    "The pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size $2 \\times 2$ applied with a stride of $2$ downsamples at every depth slice in the input by $2$ along both width and height. \n",
    "\n",
    "In addition to max pooling, the pooling units can use other functions, such as average pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which works better in practice. Due to the aggressive reduction in the size of the representation, the trend is towards using smaller filters or discarding the pooling layer altogether.\n",
    "\n",
    "#### Activation function (nonlinearity)\n",
    "ReLU is the abbreviation of Rectified Linear Units. This layer applies the non-saturating activation function \n",
    "$\\sigma(x) = \\max(0,x).$ It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer. Other functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent and the sigmoid function $f(x)=(1+e^{-x})^{-1}$. ReLU is often preferred to other functions, because it trains the neural network several times faster without a significant penalty to generalisation accuracy. Also there are several variations such as LeakyReLU or ELU.\n",
    "\n",
    "#### Fully connected layer (dense layer)\n",
    "Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.\n",
    "\n",
    "It is worth noting that the only difference between FC (fully connected layer) and conv layer is that the neurons in the conv layer are connected only to a local region in the input, and that many of the neurons in a conv volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it is possible to convert from FC to conv layers.\n",
    "\n",
    "For example, an FC layer with output size $K$ that is looking at some input volume of size $S \\times S \\times F$ can be equivalently expressed as a conv layer with size $S$, padding $0$, stride $1$ and number of filters equals $K$.\n",
    "\n",
    "#### Loss function\n",
    "The loss layer specifies how training penalizes the deviation between the predicted and true labels and is normally the final layer. Various loss functions appropriate for different tasks may be used there. [Softmax](https://en.wikipedia.org/wiki/Softmax_function) is used for predicting a single class of $K$ mutually exclusive classes. [Sigmoid cross-entropy](https://www.tensorflow.org/api_docs/python/tf/losses/sigmoid_cross_entropy) is used for predicting $K$ independent probability values. [Euclidean loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is used for regressing to real-valued labels.\n",
    "\n",
    "\n",
    "#### Exercises\n",
    "1. Download dataset from [kaggle](https://www.kaggle.com/c/ch-2017).\n",
    "2. Suggest some your net architecture (start with something really simple).\n",
    "3. What quality do you achieve?\n",
    "4. Can you transform VGG model for your problem?\n",
    "5. What is your score?\n",
    "6. Imagine that your conv net makes forward path. How can you estimate your memory consumption?\n",
    "7. Now you make backpropagation step. Why does it require much more memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import utils\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def cut(image):\n",
    "    height, width = np.shape(image)\n",
    "    diff = abs(height - width)\n",
    "    if (height > width):\n",
    "        blank = np.ones((height, diff // 2)) * 255\n",
    "        image = np.hstack((blank, image, blank))\n",
    "        \n",
    "    elif (width > height):\n",
    "        blank = np.ones((diff // 2, width)) * 255\n",
    "        image = np.vstack((blank, image, blank))\n",
    "        \n",
    "            \n",
    "            \n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = (image / 255 * 2) - 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data1 = np.load('train-1.npy')\n",
    "train_data2 = np.load('train-2.npy')\n",
    "train_data3 = np.load('train-3.npy')\n",
    "test_data = np.load('train-4.npy')\n",
    "\n",
    "train_data = np.vstack((train_data1, train_data2, train_data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tags = deepcopy(train_data[:,1])\n",
    "train_target = deepcopy(train_data[:,1])\n",
    "train_tags.sort()\n",
    "train_tags = np.unique(train_tags)\n",
    "\n",
    "test_tags = deepcopy(test_data[:,1])\n",
    "test_target = deepcopy(test_data[:,1])\n",
    "test_tags.sort()\n",
    "test_tags = np.unique(test_tags)\n",
    "\n",
    "train_classes = dict()\n",
    "test_classes = dict()\n",
    "for i in range(1000):\n",
    "    train_classes[train_tags[i]] = i\n",
    "    test_classes[test_tags[i]] = i\n",
    "    \n",
    "    \n",
    "#print(train_data[1])\n",
    "train_pics =  deepcopy(train_data[:,0])\n",
    "test_pics =  deepcopy(test_data[:,0])\n",
    "#print(train_pics)\n",
    "for i in range(len(train_pics)):\n",
    "    #print(train_pics[i])\n",
    "    train_pics[i] = cut(train_pics[i])\n",
    "    train_pics[i] = torch.from_numpy(train_pics[i]).float()\n",
    "    train_target[i] = train_classes[train_target[i]]\n",
    "    \n",
    "    \n",
    "for i in range(len(test_pics)):\n",
    "    test_pics[i] = cut(test_pics[i])\n",
    "    test_pics[i] = torch.from_numpy(test_pics[i]).float()\n",
    "    test_target[i] = test_classes[test_target[i]]\n",
    "    \n",
    "    \n",
    "\n",
    "train_pics =  torch.stack(train_pics.tolist())\n",
    "test_pics =  torch.stack(test_pics.tolist())\n",
    "\n",
    "train_target = torch.from_numpy(train_target.astype(np.long))\n",
    "test_target = torch.from_numpy(test_target.astype(np.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "lambd1 = 20\n",
    "lambd2 = 20\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    loss = 0\n",
    "    idx = 0\n",
    "    for i in range(len(train_pics) // batch_size):\n",
    "        data = train_pics[idx : idx + batch_size].cuda()\n",
    "        target = train_target[idx : idx + batch_size].cuda().long()\n",
    "        idx += batch_size\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        #print(len(output), len(target))\n",
    "\n",
    "        L1 = torch.norm(model.fc1.weight.data, 1) + torch.norm(model.fc2.weight.data, 1)\n",
    "        L2 = torch.norm(model.fc1.weight.data ** 2, 2) + torch.norm(model.fc2.weight.data ** 2, 2)\n",
    "        loss = F.cross_entropy(output, target) + lambd1 * L1 + lambd2 * L2\n",
    "        #loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, len(train_pics) // batch_size * i, len(train_data),\n",
    "                100. * batch_size * i / len(train_data), loss.data.item()))\n",
    "    else:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, len(train_pics) // batch_size * i, len(train_data),\n",
    "            100. * batch_size * i / len(train_data), loss.data.item()))  \n",
    "\n",
    "        \n",
    "def test(log):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        idx = 0\n",
    "        for i in range(len(test_data) // batch_size):\n",
    "            data = test_pics[idx : idx + batch_size].cuda()\n",
    "            target = test_target[idx : idx + batch_size].cuda().long()\n",
    "            idx += batch_size\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            test_correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "            #print(i)\n",
    "            #print(pred.eq(target.data.view_as(pred)).sum())\n",
    "            \n",
    "        test_correct_percent = 100. * test_correct / len(test_data)\n",
    "\n",
    "        \n",
    "    \n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_correct, len(test_pics), test_correct_percent))\n",
    "    \n",
    "    \n",
    "    log['test'].append((test_correct_percent))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
    "        # 4 x 48 x 48\n",
    "        self.Mpool = nn.MaxPool2d(2, 2)\n",
    "        # 4 x 12 x 12\n",
    "        self.Apool = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        # 16 x 12 x 12\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 1500)\n",
    "        self.fc2 = nn.Linear(1500, 1000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 48, 48)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/249740 (0%)]\tLoss: 1107305.875000\n",
      "Train Epoch: 1 [249700/249740 (4%)]\tLoss: 1360198.875000\n",
      "Train Epoch: 1 [499400/249740 (8%)]\tLoss: 1905040.375000\n",
      "Train Epoch: 1 [749100/249740 (12%)]\tLoss: 2302757.250000\n",
      "Train Epoch: 1 [998800/249740 (16%)]\tLoss: 2583057.500000\n",
      "Train Epoch: 1 [1248500/249740 (20%)]\tLoss: 2761861.250000\n",
      "Train Epoch: 1 [1498200/249740 (24%)]\tLoss: 2898725.000000\n",
      "Train Epoch: 1 [1747900/249740 (28%)]\tLoss: 3012372.500000\n",
      "Train Epoch: 1 [1997600/249740 (32%)]\tLoss: 3116381.500000\n",
      "Train Epoch: 1 [2247300/249740 (36%)]\tLoss: 3201483.250000\n",
      "Train Epoch: 1 [2497000/249740 (40%)]\tLoss: 3273445.250000\n",
      "Train Epoch: 1 [2746700/249740 (44%)]\tLoss: 3344453.000000\n",
      "Train Epoch: 1 [2996400/249740 (48%)]\tLoss: 3404059.500000\n",
      "Train Epoch: 1 [3246100/249740 (52%)]\tLoss: 3472292.500000\n",
      "Train Epoch: 1 [3495800/249740 (56%)]\tLoss: 3534177.000000\n",
      "Train Epoch: 1 [3745500/249740 (60%)]\tLoss: 3592566.750000\n",
      "Train Epoch: 1 [3995200/249740 (64%)]\tLoss: 3645648.500000\n",
      "Train Epoch: 1 [4244900/249740 (68%)]\tLoss: 3702555.250000\n",
      "Train Epoch: 1 [4494600/249740 (72%)]\tLoss: 3763761.250000\n",
      "Train Epoch: 1 [4744300/249740 (76%)]\tLoss: 3817331.000000\n",
      "Train Epoch: 1 [4994000/249740 (80%)]\tLoss: 3867078.750000\n",
      "Train Epoch: 1 [5243700/249740 (84%)]\tLoss: 3919021.750000\n",
      "Train Epoch: 1 [5493400/249740 (88%)]\tLoss: 3972103.500000\n",
      "Train Epoch: 1 [5743100/249740 (92%)]\tLoss: 4021181.250000\n",
      "Train Epoch: 1 [5992800/249740 (96%)]\tLoss: 4077314.500000\n",
      "Train Epoch: 1 [6232512/249740 (100%)]\tLoss: 4126155.000000\n",
      "\n",
      "Test set: Accuracy: 74459/83247 (89%)\n",
      "Train Epoch: 2 [0/249740 (0%)]\tLoss: 4126644.750000\n",
      "Train Epoch: 2 [249700/249740 (4%)]\tLoss: 4144989.250000\n",
      "Train Epoch: 2 [499400/249740 (8%)]\tLoss: 4162969.000000\n",
      "Train Epoch: 2 [749100/249740 (12%)]\tLoss: 4185804.750000\n",
      "Train Epoch: 2 [998800/249740 (16%)]\tLoss: 4213093.000000\n",
      "Train Epoch: 2 [1248500/249740 (20%)]\tLoss: 4251556.500000\n",
      "Train Epoch: 2 [1498200/249740 (24%)]\tLoss: 4304341.000000\n",
      "Train Epoch: 2 [1747900/249740 (28%)]\tLoss: 4365101.500000\n",
      "Train Epoch: 2 [1997600/249740 (32%)]\tLoss: 4424663.500000\n",
      "Train Epoch: 2 [2247300/249740 (36%)]\tLoss: 4490238.000000\n",
      "Train Epoch: 2 [2497000/249740 (40%)]\tLoss: 4542532.000000\n",
      "Train Epoch: 2 [2746700/249740 (44%)]\tLoss: 4596434.000000\n",
      "Train Epoch: 2 [2996400/249740 (48%)]\tLoss: 4644573.000000\n",
      "Train Epoch: 2 [3246100/249740 (52%)]\tLoss: 4698940.500000\n",
      "Train Epoch: 2 [3495800/249740 (56%)]\tLoss: 4753934.500000\n",
      "Train Epoch: 2 [3745500/249740 (60%)]\tLoss: 4801226.000000\n",
      "Train Epoch: 2 [3995200/249740 (64%)]\tLoss: 4854616.000000\n",
      "Train Epoch: 2 [4244900/249740 (68%)]\tLoss: 4905708.000000\n",
      "Train Epoch: 2 [4494600/249740 (72%)]\tLoss: 4961452.000000\n",
      "Train Epoch: 2 [4744300/249740 (76%)]\tLoss: 5016020.500000\n",
      "Train Epoch: 2 [4994000/249740 (80%)]\tLoss: 5061698.500000\n",
      "Train Epoch: 2 [5243700/249740 (84%)]\tLoss: 5111007.500000\n",
      "Train Epoch: 2 [5493400/249740 (88%)]\tLoss: 5164797.500000\n",
      "Train Epoch: 2 [5743100/249740 (92%)]\tLoss: 5217020.500000\n",
      "Train Epoch: 2 [5992800/249740 (96%)]\tLoss: 5268853.500000\n",
      "Train Epoch: 2 [6232512/249740 (100%)]\tLoss: 5318352.000000\n",
      "\n",
      "Test set: Accuracy: 76078/83247 (91%)\n",
      "Train Epoch: 3 [0/249740 (0%)]\tLoss: 5318829.500000\n",
      "Train Epoch: 3 [249700/249740 (4%)]\tLoss: 5335831.000000\n",
      "Train Epoch: 3 [499400/249740 (8%)]\tLoss: 5353804.500000\n",
      "Train Epoch: 3 [749100/249740 (12%)]\tLoss: 5372692.500000\n",
      "Train Epoch: 3 [998800/249740 (16%)]\tLoss: 5401496.000000\n",
      "Train Epoch: 3 [1248500/249740 (20%)]\tLoss: 5437470.000000\n",
      "Train Epoch: 3 [1498200/249740 (24%)]\tLoss: 5485376.000000\n",
      "Train Epoch: 3 [1747900/249740 (28%)]\tLoss: 5539154.000000\n",
      "Train Epoch: 3 [1997600/249740 (32%)]\tLoss: 5598357.000000\n",
      "Train Epoch: 3 [2247300/249740 (36%)]\tLoss: 5651677.000000\n",
      "Train Epoch: 3 [2497000/249740 (40%)]\tLoss: 5702550.500000\n",
      "Train Epoch: 3 [2746700/249740 (44%)]\tLoss: 5751438.500000\n",
      "Train Epoch: 3 [2996400/249740 (48%)]\tLoss: 5797254.000000\n",
      "Train Epoch: 3 [3246100/249740 (52%)]\tLoss: 5844631.000000\n",
      "Train Epoch: 3 [3495800/249740 (56%)]\tLoss: 5897940.500000\n",
      "Train Epoch: 3 [3745500/249740 (60%)]\tLoss: 5937957.000000\n",
      "Train Epoch: 3 [3995200/249740 (64%)]\tLoss: 5984315.500000\n",
      "Train Epoch: 3 [4244900/249740 (68%)]\tLoss: 6018922.000000\n",
      "Train Epoch: 3 [4494600/249740 (72%)]\tLoss: 6063897.000000\n",
      "Train Epoch: 3 [4744300/249740 (76%)]\tLoss: 6106739.000000\n",
      "Train Epoch: 3 [4994000/249740 (80%)]\tLoss: 6142564.500000\n",
      "Train Epoch: 3 [5243700/249740 (84%)]\tLoss: 6174872.500000\n",
      "Train Epoch: 3 [5493400/249740 (88%)]\tLoss: 6220108.500000\n",
      "Train Epoch: 3 [5743100/249740 (92%)]\tLoss: 6265566.000000\n",
      "Train Epoch: 3 [5992800/249740 (96%)]\tLoss: 6302792.500000\n",
      "Train Epoch: 3 [6232512/249740 (100%)]\tLoss: 6341685.000000\n",
      "\n",
      "Test set: Accuracy: 77191/83247 (92%)\n",
      "Train Epoch: 4 [0/249740 (0%)]\tLoss: 6341991.000000\n",
      "Train Epoch: 4 [249700/249740 (4%)]\tLoss: 6355598.500000\n",
      "Train Epoch: 4 [499400/249740 (8%)]\tLoss: 6369257.500000\n",
      "Train Epoch: 4 [749100/249740 (12%)]\tLoss: 6386882.000000\n",
      "Train Epoch: 4 [998800/249740 (16%)]\tLoss: 6406699.500000\n",
      "Train Epoch: 4 [1248500/249740 (20%)]\tLoss: 6432985.000000\n",
      "Train Epoch: 4 [1498200/249740 (24%)]\tLoss: 6467004.000000\n",
      "Train Epoch: 4 [1747900/249740 (28%)]\tLoss: 6504842.500000\n",
      "Train Epoch: 4 [1997600/249740 (32%)]\tLoss: 6550825.500000\n",
      "Train Epoch: 4 [2247300/249740 (36%)]\tLoss: 6597688.500000\n",
      "Train Epoch: 4 [2497000/249740 (40%)]\tLoss: 6650577.500000\n",
      "Train Epoch: 4 [2746700/249740 (44%)]\tLoss: 6696049.000000\n",
      "Train Epoch: 4 [2996400/249740 (48%)]\tLoss: 6738052.500000\n",
      "Train Epoch: 4 [3246100/249740 (52%)]\tLoss: 6773561.500000\n",
      "Train Epoch: 4 [3495800/249740 (56%)]\tLoss: 6810020.000000\n",
      "Train Epoch: 4 [3745500/249740 (60%)]\tLoss: 6847223.000000\n",
      "Train Epoch: 4 [3995200/249740 (64%)]\tLoss: 6881144.000000\n",
      "Train Epoch: 4 [4244900/249740 (68%)]\tLoss: 6916806.500000\n",
      "Train Epoch: 4 [4494600/249740 (72%)]\tLoss: 6963394.500000\n",
      "Train Epoch: 4 [4744300/249740 (76%)]\tLoss: 6998085.000000\n",
      "Train Epoch: 4 [4994000/249740 (80%)]\tLoss: 7030581.500000\n",
      "Train Epoch: 4 [5243700/249740 (84%)]\tLoss: 7063949.000000\n",
      "Train Epoch: 4 [5493400/249740 (88%)]\tLoss: 7096150.000000\n",
      "Train Epoch: 4 [5743100/249740 (92%)]\tLoss: 7126536.000000\n",
      "Train Epoch: 4 [5992800/249740 (96%)]\tLoss: 7159343.500000\n",
      "Train Epoch: 4 [6232512/249740 (100%)]\tLoss: 7192036.000000\n",
      "\n",
      "Test set: Accuracy: 76743/83247 (92%)\n",
      "Train Epoch: 5 [0/249740 (0%)]\tLoss: 7192365.000000\n",
      "Train Epoch: 5 [249700/249740 (4%)]\tLoss: 7207673.000000\n",
      "Train Epoch: 5 [499400/249740 (8%)]\tLoss: 7218441.000000\n",
      "Train Epoch: 5 [749100/249740 (12%)]\tLoss: 7229075.000000\n",
      "Train Epoch: 5 [998800/249740 (16%)]\tLoss: 7245722.000000\n",
      "Train Epoch: 5 [1248500/249740 (20%)]\tLoss: 7268195.500000\n",
      "Train Epoch: 5 [1498200/249740 (24%)]\tLoss: 7291966.500000\n",
      "Train Epoch: 5 [1747900/249740 (28%)]\tLoss: 7321118.500000\n",
      "Train Epoch: 5 [1997600/249740 (32%)]\tLoss: 7364071.500000\n",
      "Train Epoch: 5 [2247300/249740 (36%)]\tLoss: 7411866.500000\n",
      "Train Epoch: 5 [2497000/249740 (40%)]\tLoss: 7459915.500000\n",
      "Train Epoch: 5 [2746700/249740 (44%)]\tLoss: 7491994.000000\n",
      "Train Epoch: 5 [2996400/249740 (48%)]\tLoss: 7525618.000000\n",
      "Train Epoch: 5 [3246100/249740 (52%)]\tLoss: 7560985.000000\n",
      "Train Epoch: 5 [3495800/249740 (56%)]\tLoss: 7593313.500000\n",
      "Train Epoch: 5 [3745500/249740 (60%)]\tLoss: 7629931.500000\n",
      "Train Epoch: 5 [3995200/249740 (64%)]\tLoss: 7666570.500000\n",
      "Train Epoch: 5 [4244900/249740 (68%)]\tLoss: 7699169.000000\n",
      "Train Epoch: 5 [4494600/249740 (72%)]\tLoss: 7730463.000000\n",
      "Train Epoch: 5 [4744300/249740 (76%)]\tLoss: 7759985.000000\n",
      "Train Epoch: 5 [4994000/249740 (80%)]\tLoss: 7795205.500000\n",
      "Train Epoch: 5 [5243700/249740 (84%)]\tLoss: 7829536.500000\n",
      "Train Epoch: 5 [5493400/249740 (88%)]\tLoss: 7858932.500000\n",
      "Train Epoch: 5 [5743100/249740 (92%)]\tLoss: 7896615.000000\n",
      "Train Epoch: 5 [5992800/249740 (96%)]\tLoss: 7925470.500000\n",
      "Train Epoch: 5 [6232512/249740 (100%)]\tLoss: 7956151.000000\n",
      "\n",
      "Test set: Accuracy: 77408/83247 (92%)\n",
      "Train Epoch: 6 [0/249740 (0%)]\tLoss: 7956416.500000\n",
      "Train Epoch: 6 [249700/249740 (4%)]\tLoss: 7964647.000000\n",
      "Train Epoch: 6 [499400/249740 (8%)]\tLoss: 7972567.500000\n",
      "Train Epoch: 6 [749100/249740 (12%)]\tLoss: 7985892.000000\n",
      "Train Epoch: 6 [998800/249740 (16%)]\tLoss: 8007192.000000\n",
      "Train Epoch: 6 [1248500/249740 (20%)]\tLoss: 8029987.000000\n",
      "Train Epoch: 6 [1498200/249740 (24%)]\tLoss: 8055945.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1747900/249740 (28%)]\tLoss: 8082813.000000\n",
      "Train Epoch: 6 [1997600/249740 (32%)]\tLoss: 8113402.500000\n",
      "Train Epoch: 6 [2247300/249740 (36%)]\tLoss: 8148462.500000\n",
      "Train Epoch: 6 [2497000/249740 (40%)]\tLoss: 8184977.500000\n",
      "Train Epoch: 6 [2746700/249740 (44%)]\tLoss: 8215774.000000\n",
      "Train Epoch: 6 [2996400/249740 (48%)]\tLoss: 8244345.500000\n",
      "Train Epoch: 6 [3246100/249740 (52%)]\tLoss: 8274743.000000\n",
      "Train Epoch: 6 [3495800/249740 (56%)]\tLoss: 8305873.500000\n",
      "Train Epoch: 6 [3745500/249740 (60%)]\tLoss: 8331660.000000\n",
      "Train Epoch: 6 [3995200/249740 (64%)]\tLoss: 8362411.500000\n",
      "Train Epoch: 6 [4244900/249740 (68%)]\tLoss: 8391430.000000\n",
      "Train Epoch: 6 [4494600/249740 (72%)]\tLoss: 8419751.000000\n",
      "Train Epoch: 6 [4744300/249740 (76%)]\tLoss: 8453962.000000\n",
      "Train Epoch: 6 [4994000/249740 (80%)]\tLoss: 8484561.000000\n",
      "Train Epoch: 6 [5243700/249740 (84%)]\tLoss: 8512398.000000\n",
      "Train Epoch: 6 [5493400/249740 (88%)]\tLoss: 8540322.000000\n",
      "Train Epoch: 6 [5743100/249740 (92%)]\tLoss: 8577047.000000\n",
      "Train Epoch: 6 [5992800/249740 (96%)]\tLoss: 8605470.000000\n",
      "Train Epoch: 6 [6232512/249740 (100%)]\tLoss: 8634559.000000\n",
      "\n",
      "Test set: Accuracy: 77795/83247 (93%)\n",
      "Train Epoch: 7 [0/249740 (0%)]\tLoss: 8634967.000000\n",
      "Train Epoch: 7 [249700/249740 (4%)]\tLoss: 8645153.000000\n",
      "Train Epoch: 7 [499400/249740 (8%)]\tLoss: 8651917.000000\n",
      "Train Epoch: 7 [749100/249740 (12%)]\tLoss: 8659629.000000\n",
      "Train Epoch: 7 [998800/249740 (16%)]\tLoss: 8669994.000000\n",
      "Train Epoch: 7 [1248500/249740 (20%)]\tLoss: 8690855.000000\n",
      "Train Epoch: 7 [1498200/249740 (24%)]\tLoss: 8712808.000000\n",
      "Train Epoch: 7 [1747900/249740 (28%)]\tLoss: 8746081.000000\n",
      "Train Epoch: 7 [1997600/249740 (32%)]\tLoss: 8776534.000000\n",
      "Train Epoch: 7 [2247300/249740 (36%)]\tLoss: 8808595.000000\n",
      "Train Epoch: 7 [2497000/249740 (40%)]\tLoss: 8839294.000000\n",
      "Train Epoch: 7 [2746700/249740 (44%)]\tLoss: 8870948.000000\n",
      "Train Epoch: 7 [2996400/249740 (48%)]\tLoss: 8895268.000000\n",
      "Train Epoch: 7 [3246100/249740 (52%)]\tLoss: 8931616.000000\n",
      "Train Epoch: 7 [3495800/249740 (56%)]\tLoss: 8967190.000000\n",
      "Train Epoch: 7 [3745500/249740 (60%)]\tLoss: 8999595.000000\n",
      "Train Epoch: 7 [3995200/249740 (64%)]\tLoss: 9028345.000000\n",
      "Train Epoch: 7 [4244900/249740 (68%)]\tLoss: 9056417.000000\n",
      "Train Epoch: 7 [4494600/249740 (72%)]\tLoss: 9083543.000000\n",
      "Train Epoch: 7 [4744300/249740 (76%)]\tLoss: 9107170.000000\n",
      "Train Epoch: 7 [4994000/249740 (80%)]\tLoss: 9134525.000000\n",
      "Train Epoch: 7 [5243700/249740 (84%)]\tLoss: 9159930.000000\n",
      "Train Epoch: 7 [5493400/249740 (88%)]\tLoss: 9184888.000000\n",
      "Train Epoch: 7 [5743100/249740 (92%)]\tLoss: 9205498.000000\n",
      "Train Epoch: 7 [5992800/249740 (96%)]\tLoss: 9232705.000000\n",
      "Train Epoch: 7 [6232512/249740 (100%)]\tLoss: 9263997.000000\n",
      "\n",
      "Test set: Accuracy: 77320/83247 (92%)\n",
      "Train Epoch: 8 [0/249740 (0%)]\tLoss: 9264284.000000\n",
      "Train Epoch: 8 [249700/249740 (4%)]\tLoss: 9276168.000000\n",
      "Train Epoch: 8 [499400/249740 (8%)]\tLoss: 9286609.000000\n",
      "Train Epoch: 8 [749100/249740 (12%)]\tLoss: 9300032.000000\n",
      "Train Epoch: 8 [998800/249740 (16%)]\tLoss: 9314778.000000\n",
      "Train Epoch: 8 [1248500/249740 (20%)]\tLoss: 9331936.000000\n",
      "Train Epoch: 8 [1498200/249740 (24%)]\tLoss: 9354898.000000\n",
      "Train Epoch: 8 [1747900/249740 (28%)]\tLoss: 9383786.000000\n",
      "Train Epoch: 8 [1997600/249740 (32%)]\tLoss: 9410812.000000\n",
      "Train Epoch: 8 [2247300/249740 (36%)]\tLoss: 9437538.000000\n",
      "Train Epoch: 8 [2497000/249740 (40%)]\tLoss: 9463143.000000\n",
      "Train Epoch: 8 [2746700/249740 (44%)]\tLoss: 9486167.000000\n",
      "Train Epoch: 8 [2996400/249740 (48%)]\tLoss: 9515244.000000\n",
      "Train Epoch: 8 [3246100/249740 (52%)]\tLoss: 9551046.000000\n",
      "Train Epoch: 8 [3495800/249740 (56%)]\tLoss: 9577386.000000\n",
      "Train Epoch: 8 [3745500/249740 (60%)]\tLoss: 9607110.000000\n",
      "Train Epoch: 8 [3995200/249740 (64%)]\tLoss: 9634353.000000\n",
      "Train Epoch: 8 [4244900/249740 (68%)]\tLoss: 9658628.000000\n",
      "Train Epoch: 8 [4494600/249740 (72%)]\tLoss: 9682927.000000\n",
      "Train Epoch: 8 [4744300/249740 (76%)]\tLoss: 9706396.000000\n",
      "Train Epoch: 8 [4994000/249740 (80%)]\tLoss: 9733434.000000\n",
      "Train Epoch: 8 [5243700/249740 (84%)]\tLoss: 9756603.000000\n",
      "Train Epoch: 8 [5493400/249740 (88%)]\tLoss: 9780214.000000\n",
      "Train Epoch: 8 [5743100/249740 (92%)]\tLoss: 9801048.000000\n",
      "Train Epoch: 8 [5992800/249740 (96%)]\tLoss: 9823940.000000\n",
      "Train Epoch: 8 [6232512/249740 (100%)]\tLoss: 9849563.000000\n",
      "\n",
      "Test set: Accuracy: 77433/83247 (93%)\n",
      "Train Epoch: 9 [0/249740 (0%)]\tLoss: 9849891.000000\n",
      "Train Epoch: 9 [249700/249740 (4%)]\tLoss: 9857774.000000\n",
      "Train Epoch: 9 [499400/249740 (8%)]\tLoss: 9863519.000000\n",
      "Train Epoch: 9 [749100/249740 (12%)]\tLoss: 9869768.000000\n",
      "Train Epoch: 9 [998800/249740 (16%)]\tLoss: 9881145.000000\n",
      "Train Epoch: 9 [1248500/249740 (20%)]\tLoss: 9892229.000000\n",
      "Train Epoch: 9 [1498200/249740 (24%)]\tLoss: 9911353.000000\n",
      "Train Epoch: 9 [1747900/249740 (28%)]\tLoss: 9933823.000000\n",
      "Train Epoch: 9 [1997600/249740 (32%)]\tLoss: 9959042.000000\n",
      "Train Epoch: 9 [2247300/249740 (36%)]\tLoss: 9986997.000000\n",
      "Train Epoch: 9 [2497000/249740 (40%)]\tLoss: 10012897.000000\n",
      "Train Epoch: 9 [2746700/249740 (44%)]\tLoss: 10036241.000000\n",
      "Train Epoch: 9 [2996400/249740 (48%)]\tLoss: 10060728.000000\n",
      "Train Epoch: 9 [3246100/249740 (52%)]\tLoss: 10087121.000000\n",
      "Train Epoch: 9 [3495800/249740 (56%)]\tLoss: 10109392.000000\n",
      "Train Epoch: 9 [3745500/249740 (60%)]\tLoss: 10133177.000000\n",
      "Train Epoch: 9 [3995200/249740 (64%)]\tLoss: 10157742.000000\n",
      "Train Epoch: 9 [4244900/249740 (68%)]\tLoss: 10177895.000000\n",
      "Train Epoch: 9 [4494600/249740 (72%)]\tLoss: 10199769.000000\n",
      "Train Epoch: 9 [4744300/249740 (76%)]\tLoss: 10223390.000000\n",
      "Train Epoch: 9 [4994000/249740 (80%)]\tLoss: 10250434.000000\n",
      "Train Epoch: 9 [5243700/249740 (84%)]\tLoss: 10278201.000000\n",
      "Train Epoch: 9 [5493400/249740 (88%)]\tLoss: 10308417.000000\n",
      "Train Epoch: 9 [5743100/249740 (92%)]\tLoss: 10340501.000000\n",
      "Train Epoch: 9 [5992800/249740 (96%)]\tLoss: 10369745.000000\n",
      "Train Epoch: 9 [6232512/249740 (100%)]\tLoss: 10396751.000000\n",
      "\n",
      "Test set: Accuracy: 77746/83247 (93%)\n",
      "Train Epoch: 10 [0/249740 (0%)]\tLoss: 10397008.000000\n",
      "Train Epoch: 10 [249700/249740 (4%)]\tLoss: 10405103.000000\n",
      "Train Epoch: 10 [499400/249740 (8%)]\tLoss: 10412418.000000\n",
      "Train Epoch: 10 [749100/249740 (12%)]\tLoss: 10420228.000000\n",
      "Train Epoch: 10 [998800/249740 (16%)]\tLoss: 10430812.000000\n",
      "Train Epoch: 10 [1248500/249740 (20%)]\tLoss: 10445898.000000\n",
      "Train Epoch: 10 [1498200/249740 (24%)]\tLoss: 10465735.000000\n",
      "Train Epoch: 10 [1747900/249740 (28%)]\tLoss: 10484208.000000\n",
      "Train Epoch: 10 [1997600/249740 (32%)]\tLoss: 10507577.000000\n",
      "Train Epoch: 10 [2247300/249740 (36%)]\tLoss: 10533947.000000\n",
      "Train Epoch: 10 [2497000/249740 (40%)]\tLoss: 10565855.000000\n",
      "Train Epoch: 10 [2746700/249740 (44%)]\tLoss: 10592300.000000\n",
      "Train Epoch: 10 [2996400/249740 (48%)]\tLoss: 10618724.000000\n",
      "Train Epoch: 10 [3246100/249740 (52%)]\tLoss: 10641712.000000\n",
      "Train Epoch: 10 [3495800/249740 (56%)]\tLoss: 10666405.000000\n",
      "Train Epoch: 10 [3745500/249740 (60%)]\tLoss: 10689066.000000\n",
      "Train Epoch: 10 [3995200/249740 (64%)]\tLoss: 10706779.000000\n",
      "Train Epoch: 10 [4244900/249740 (68%)]\tLoss: 10726725.000000\n",
      "Train Epoch: 10 [4494600/249740 (72%)]\tLoss: 10745219.000000\n",
      "Train Epoch: 10 [4744300/249740 (76%)]\tLoss: 10766449.000000\n",
      "Train Epoch: 10 [4994000/249740 (80%)]\tLoss: 10788365.000000\n",
      "Train Epoch: 10 [5243700/249740 (84%)]\tLoss: 10812398.000000\n",
      "Train Epoch: 10 [5493400/249740 (88%)]\tLoss: 10840920.000000\n",
      "Train Epoch: 10 [5743100/249740 (92%)]\tLoss: 10871425.000000\n",
      "Train Epoch: 10 [5992800/249740 (96%)]\tLoss: 10897350.000000\n",
      "Train Epoch: 10 [6232512/249740 (100%)]\tLoss: 10917043.000000\n",
      "\n",
      "Test set: Accuracy: 77710/83247 (93%)\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "err_log = {'test': [], 'train': []}\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    train(i + 1)\n",
    "    test(err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model reaches over 93% accuracy on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularization\n",
    "Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. It is possible to use various types of regularization for conv nets. You are already familiar with the classic $L1$ and $L2$ regularization, consider a more specific techniques.\n",
    "\n",
    "#### Early stopping\n",
    "One more method to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. Also it is common solution to slowly decreace learning rate.\n",
    "\n",
    "#### Dropout\n",
    "Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is [dropout](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf). At each training stage, individual nodes are either \"dropped out\" of the net with probability $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages, the probability that a hidden node will be dropped is usually $0.5$, for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n",
    "\n",
    "At testing time after training has finished, we would ideally like to find a sample average of all possible $2^{n}$ dropped-out networks, unfortunately this is unfeasible for large values of $n$. However, we can find an approximation by using the full network with each node's output weighted by a factor of $p$, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates $2^{n}$ neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n",
    "\n",
    "By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes model combination practical, even for deep neural nets. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n",
    "\n",
    "#### Stochastic pooling\n",
    "A major drawback to dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n",
    "\n",
    "In [stochastic pooling](https://arxiv.org/abs/1301.3557), the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. The approach is hyperparameter free and can be combined with other regularization approaches, such as dropout and data augmentation.\n",
    "\n",
    "An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent MNIST performance. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n",
    "\n",
    "#### Batch normalization\n",
    "[Batch normalization](https://arxiv.org/pdf/1502.03167.pdf) is a method for improving the performance and stability of neural networks, and also makes more sophisticated deep learning architectures work in practice.\n",
    "The idea is to normalise the inputs of each layer in such a way that they have a mean output activation of zero and standard deviation of one. This is analogous to how the inputs to networks are standardised.\n",
    "\n",
    "How does this help? We know that normalising the inputs to a network helps it learn. But a network is just a series of layers, where the output of one layer becomes the input to the next. That means we can think of any layer in a neural network as the first layer of a smaller subsequent network. Thought of as a series of neural networks feeding into each other, we normalising the output of one layer before applying the activation function, and then feed it into the following layer. It’s called \"batch\" normalization because during training, we normalise the activations of the previous layer for each batch, i.e. apply a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "\n",
    "1. Networks train faster – whilst each training iteration will be slower because of the extra normalisation calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall.\n",
    "2. Allows higher learning rates — gradient descent usually requires small learning rates for the network to converge. As networks get deeper, gradients get smaller during back propagation, and so require even more iterations. Using batch normalisation allows much higher learning rates, increasing the speed at which networks train.\n",
    "3. Makes weights easier to initialise — weight initialisation can be difficult, especially when creating deeper networks. Batch normalisation helps reduce the sensitivity to the initial starting weights.\n",
    "4. Makes more activation functions viable — some activation functions don’t work well in certain situations. Sigmoids lose their gradient quickly, which means they can’t be used in deep networks, and ReLUs often die out during training (stop learning completely), so we must be careful about the range of values fed into them.\n",
    "5. Provides some regularisation — batch normalisation adds a little noise to your network, and in some cases, (e.g. Inception modules) it has been shown to work as well as dropout. You can consider batch normalisation as a bit of extra regularization, allowing you to reduce some of the dropout you might add to a network.\n",
    "\n",
    "#### Exercises\n",
    "1. Try to use regularization that you like more (batch normalization is strongly recommended).\n",
    "2. Does it help to improve the quality of classification? What methods do you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
    "        # 4 x 48 x 48\n",
    "        self.Mpool = nn.MaxPool2d(2, 2)\n",
    "        # 4 x 12 x 12\n",
    "        self.Apool = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        # 16 x 12 x 12\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 1500)\n",
    "        self.fc2 = nn.Linear(1500, 1000)\n",
    "        self.norm = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 48, 48)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.Mpool(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/249740 (0%)]\tLoss: 1107282.125000\n",
      "Train Epoch: 1 [249700/249740 (4%)]\tLoss: 1824535.000000\n",
      "Train Epoch: 1 [499400/249740 (8%)]\tLoss: 2172596.750000\n",
      "Train Epoch: 1 [749100/249740 (12%)]\tLoss: 2371808.000000\n",
      "Train Epoch: 1 [998800/249740 (16%)]\tLoss: 2527617.750000\n",
      "Train Epoch: 1 [1248500/249740 (20%)]\tLoss: 2661434.750000\n",
      "Train Epoch: 1 [1498200/249740 (24%)]\tLoss: 2784817.000000\n",
      "Train Epoch: 1 [1747900/249740 (28%)]\tLoss: 2905478.500000\n",
      "Train Epoch: 1 [1997600/249740 (32%)]\tLoss: 3019082.500000\n",
      "Train Epoch: 1 [2247300/249740 (36%)]\tLoss: 3116895.000000\n",
      "Train Epoch: 1 [2497000/249740 (40%)]\tLoss: 3203220.000000\n",
      "Train Epoch: 1 [2746700/249740 (44%)]\tLoss: 3280733.500000\n",
      "Train Epoch: 1 [2996400/249740 (48%)]\tLoss: 3350951.750000\n",
      "Train Epoch: 1 [3246100/249740 (52%)]\tLoss: 3421312.750000\n",
      "Train Epoch: 1 [3495800/249740 (56%)]\tLoss: 3489205.500000\n",
      "Train Epoch: 1 [3745500/249740 (60%)]\tLoss: 3551409.000000\n",
      "Train Epoch: 1 [3995200/249740 (64%)]\tLoss: 3615679.250000\n",
      "Train Epoch: 1 [4244900/249740 (68%)]\tLoss: 3677774.750000\n",
      "Train Epoch: 1 [4494600/249740 (72%)]\tLoss: 3739886.750000\n",
      "Train Epoch: 1 [4744300/249740 (76%)]\tLoss: 3796816.750000\n",
      "Train Epoch: 1 [4994000/249740 (80%)]\tLoss: 3853010.250000\n",
      "Train Epoch: 1 [5243700/249740 (84%)]\tLoss: 3909591.000000\n",
      "Train Epoch: 1 [5493400/249740 (88%)]\tLoss: 3970254.000000\n",
      "Train Epoch: 1 [5743100/249740 (92%)]\tLoss: 4029870.750000\n",
      "Train Epoch: 1 [5992800/249740 (96%)]\tLoss: 4092095.250000\n",
      "Train Epoch: 1 [6232512/249740 (100%)]\tLoss: 4149122.500000\n",
      "\n",
      "Test set: Accuracy: 76157/83247 (91%)\n",
      "Train Epoch: 2 [0/249740 (0%)]\tLoss: 4149730.000000\n",
      "Train Epoch: 2 [249700/249740 (4%)]\tLoss: 4169824.250000\n",
      "Train Epoch: 2 [499400/249740 (8%)]\tLoss: 4187814.500000\n",
      "Train Epoch: 2 [749100/249740 (12%)]\tLoss: 4210523.500000\n",
      "Train Epoch: 2 [998800/249740 (16%)]\tLoss: 4237148.500000\n",
      "Train Epoch: 2 [1248500/249740 (20%)]\tLoss: 4271406.500000\n",
      "Train Epoch: 2 [1498200/249740 (24%)]\tLoss: 4313096.000000\n",
      "Train Epoch: 2 [1747900/249740 (28%)]\tLoss: 4367804.500000\n",
      "Train Epoch: 2 [1997600/249740 (32%)]\tLoss: 4425546.000000\n",
      "Train Epoch: 2 [2247300/249740 (36%)]\tLoss: 4486990.000000\n",
      "Train Epoch: 2 [2497000/249740 (40%)]\tLoss: 4547778.500000\n",
      "Train Epoch: 2 [2746700/249740 (44%)]\tLoss: 4603336.000000\n",
      "Train Epoch: 2 [2996400/249740 (48%)]\tLoss: 4655365.000000\n",
      "Train Epoch: 2 [3246100/249740 (52%)]\tLoss: 4703758.500000\n",
      "Train Epoch: 2 [3495800/249740 (56%)]\tLoss: 4754555.000000\n",
      "Train Epoch: 2 [3745500/249740 (60%)]\tLoss: 4809254.500000\n",
      "Train Epoch: 2 [3995200/249740 (64%)]\tLoss: 4868737.000000\n",
      "Train Epoch: 2 [4244900/249740 (68%)]\tLoss: 4927346.500000\n",
      "Train Epoch: 2 [4494600/249740 (72%)]\tLoss: 4986522.500000\n",
      "Train Epoch: 2 [4744300/249740 (76%)]\tLoss: 5040327.000000\n",
      "Train Epoch: 2 [4994000/249740 (80%)]\tLoss: 5097828.500000\n",
      "Train Epoch: 2 [5243700/249740 (84%)]\tLoss: 5149065.500000\n",
      "Train Epoch: 2 [5493400/249740 (88%)]\tLoss: 5201808.000000\n",
      "Train Epoch: 2 [5743100/249740 (92%)]\tLoss: 5257832.000000\n",
      "Train Epoch: 2 [5992800/249740 (96%)]\tLoss: 5319498.000000\n",
      "Train Epoch: 2 [6232512/249740 (100%)]\tLoss: 5374318.000000\n",
      "\n",
      "Test set: Accuracy: 77177/83247 (92%)\n",
      "Train Epoch: 3 [0/249740 (0%)]\tLoss: 5374795.500000\n",
      "Train Epoch: 3 [249700/249740 (4%)]\tLoss: 5394216.500000\n",
      "Train Epoch: 3 [499400/249740 (8%)]\tLoss: 5413275.000000\n",
      "Train Epoch: 3 [749100/249740 (12%)]\tLoss: 5434665.000000\n",
      "Train Epoch: 3 [998800/249740 (16%)]\tLoss: 5460911.500000\n",
      "Train Epoch: 3 [1248500/249740 (20%)]\tLoss: 5493105.500000\n",
      "Train Epoch: 3 [1498200/249740 (24%)]\tLoss: 5536017.500000\n",
      "Train Epoch: 3 [1747900/249740 (28%)]\tLoss: 5593570.500000\n",
      "Train Epoch: 3 [1997600/249740 (32%)]\tLoss: 5655858.500000\n",
      "Train Epoch: 3 [2247300/249740 (36%)]\tLoss: 5713040.000000\n",
      "Train Epoch: 3 [2497000/249740 (40%)]\tLoss: 5768843.000000\n",
      "Train Epoch: 3 [2746700/249740 (44%)]\tLoss: 5822824.500000\n",
      "Train Epoch: 3 [2996400/249740 (48%)]\tLoss: 5874209.500000\n",
      "Train Epoch: 3 [3246100/249740 (52%)]\tLoss: 5925960.500000\n",
      "Train Epoch: 3 [3495800/249740 (56%)]\tLoss: 5983388.500000\n",
      "Train Epoch: 3 [3745500/249740 (60%)]\tLoss: 6035120.500000\n",
      "Train Epoch: 3 [3995200/249740 (64%)]\tLoss: 6080958.000000\n",
      "Train Epoch: 3 [4244900/249740 (68%)]\tLoss: 6130623.000000\n",
      "Train Epoch: 3 [4494600/249740 (72%)]\tLoss: 6183656.500000\n",
      "Train Epoch: 3 [4744300/249740 (76%)]\tLoss: 6233647.000000\n",
      "Train Epoch: 3 [4994000/249740 (80%)]\tLoss: 6279708.000000\n",
      "Train Epoch: 3 [5243700/249740 (84%)]\tLoss: 6324967.500000\n",
      "Train Epoch: 3 [5493400/249740 (88%)]\tLoss: 6367281.500000\n",
      "Train Epoch: 3 [5743100/249740 (92%)]\tLoss: 6409840.500000\n",
      "Train Epoch: 3 [5992800/249740 (96%)]\tLoss: 6447977.500000\n",
      "Train Epoch: 3 [6232512/249740 (100%)]\tLoss: 6491612.500000\n",
      "\n",
      "Test set: Accuracy: 77529/83247 (93%)\n",
      "Train Epoch: 4 [0/249740 (0%)]\tLoss: 6492080.500000\n",
      "Train Epoch: 4 [249700/249740 (4%)]\tLoss: 6507970.000000\n",
      "Train Epoch: 4 [499400/249740 (8%)]\tLoss: 6520299.000000\n",
      "Train Epoch: 4 [749100/249740 (12%)]\tLoss: 6539511.500000\n",
      "Train Epoch: 4 [998800/249740 (16%)]\tLoss: 6561478.000000\n",
      "Train Epoch: 4 [1248500/249740 (20%)]\tLoss: 6586758.500000\n",
      "Train Epoch: 4 [1498200/249740 (24%)]\tLoss: 6617288.500000\n",
      "Train Epoch: 4 [1747900/249740 (28%)]\tLoss: 6651870.500000\n",
      "Train Epoch: 4 [1997600/249740 (32%)]\tLoss: 6696569.000000\n",
      "Train Epoch: 4 [2247300/249740 (36%)]\tLoss: 6742771.500000\n",
      "Train Epoch: 4 [2497000/249740 (40%)]\tLoss: 6795327.000000\n",
      "Train Epoch: 4 [2746700/249740 (44%)]\tLoss: 6838885.000000\n",
      "Train Epoch: 4 [2996400/249740 (48%)]\tLoss: 6880093.500000\n",
      "Train Epoch: 4 [3246100/249740 (52%)]\tLoss: 6916967.500000\n",
      "Train Epoch: 4 [3495800/249740 (56%)]\tLoss: 6960843.000000\n",
      "Train Epoch: 4 [3745500/249740 (60%)]\tLoss: 6998632.000000\n",
      "Train Epoch: 4 [3995200/249740 (64%)]\tLoss: 7043582.000000\n",
      "Train Epoch: 4 [4244900/249740 (68%)]\tLoss: 7076723.500000\n",
      "Train Epoch: 4 [4494600/249740 (72%)]\tLoss: 7109338.500000\n",
      "Train Epoch: 4 [4744300/249740 (76%)]\tLoss: 7148816.000000\n",
      "Train Epoch: 4 [4994000/249740 (80%)]\tLoss: 7189566.000000\n",
      "Train Epoch: 4 [5243700/249740 (84%)]\tLoss: 7228844.500000\n",
      "Train Epoch: 4 [5493400/249740 (88%)]\tLoss: 7262048.000000\n",
      "Train Epoch: 4 [5743100/249740 (92%)]\tLoss: 7297473.500000\n",
      "Train Epoch: 4 [5992800/249740 (96%)]\tLoss: 7339565.500000\n",
      "Train Epoch: 4 [6232512/249740 (100%)]\tLoss: 7378900.000000\n",
      "\n",
      "Test set: Accuracy: 78027/83247 (93%)\n",
      "Train Epoch: 5 [0/249740 (0%)]\tLoss: 7379285.500000\n",
      "Train Epoch: 5 [249700/249740 (4%)]\tLoss: 7395277.000000\n",
      "Train Epoch: 5 [499400/249740 (8%)]\tLoss: 7411460.500000\n",
      "Train Epoch: 5 [749100/249740 (12%)]\tLoss: 7428824.000000\n",
      "Train Epoch: 5 [998800/249740 (16%)]\tLoss: 7444314.500000\n",
      "Train Epoch: 5 [1248500/249740 (20%)]\tLoss: 7471728.000000\n",
      "Train Epoch: 5 [1498200/249740 (24%)]\tLoss: 7495103.000000\n",
      "Train Epoch: 5 [1747900/249740 (28%)]\tLoss: 7532840.000000\n",
      "Train Epoch: 5 [1997600/249740 (32%)]\tLoss: 7578411.500000\n",
      "Train Epoch: 5 [2247300/249740 (36%)]\tLoss: 7620678.000000\n",
      "Train Epoch: 5 [2497000/249740 (40%)]\tLoss: 7660849.000000\n",
      "Train Epoch: 5 [2746700/249740 (44%)]\tLoss: 7698654.000000\n",
      "Train Epoch: 5 [2996400/249740 (48%)]\tLoss: 7731277.000000\n",
      "Train Epoch: 5 [3246100/249740 (52%)]\tLoss: 7762282.000000\n",
      "Train Epoch: 5 [3495800/249740 (56%)]\tLoss: 7795447.500000\n",
      "Train Epoch: 5 [3745500/249740 (60%)]\tLoss: 7827639.000000\n",
      "Train Epoch: 5 [3995200/249740 (64%)]\tLoss: 7861142.500000\n",
      "Train Epoch: 5 [4244900/249740 (68%)]\tLoss: 7892804.000000\n",
      "Train Epoch: 5 [4494600/249740 (72%)]\tLoss: 7928457.500000\n",
      "Train Epoch: 5 [4744300/249740 (76%)]\tLoss: 7961214.500000\n",
      "Train Epoch: 5 [4994000/249740 (80%)]\tLoss: 8003149.500000\n",
      "Train Epoch: 5 [5243700/249740 (84%)]\tLoss: 8037247.000000\n",
      "Train Epoch: 5 [5493400/249740 (88%)]\tLoss: 8067868.500000\n",
      "Train Epoch: 5 [5743100/249740 (92%)]\tLoss: 8102600.500000\n",
      "Train Epoch: 5 [5992800/249740 (96%)]\tLoss: 8140231.500000\n",
      "Train Epoch: 5 [6232512/249740 (100%)]\tLoss: 8173243.000000\n",
      "\n",
      "Test set: Accuracy: 78189/83247 (93%)\n",
      "Train Epoch: 6 [0/249740 (0%)]\tLoss: 8173614.500000\n",
      "Train Epoch: 6 [249700/249740 (4%)]\tLoss: 8186533.500000\n",
      "Train Epoch: 6 [499400/249740 (8%)]\tLoss: 8196964.000000\n",
      "Train Epoch: 6 [749100/249740 (12%)]\tLoss: 8210421.000000\n",
      "Train Epoch: 6 [998800/249740 (16%)]\tLoss: 8224844.000000\n",
      "Train Epoch: 6 [1248500/249740 (20%)]\tLoss: 8241339.500000\n",
      "Train Epoch: 6 [1498200/249740 (24%)]\tLoss: 8265110.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1747900/249740 (28%)]\tLoss: 8296418.000000\n",
      "Train Epoch: 6 [1997600/249740 (32%)]\tLoss: 8336149.000000\n",
      "Train Epoch: 6 [2247300/249740 (36%)]\tLoss: 8370128.000000\n",
      "Train Epoch: 6 [2497000/249740 (40%)]\tLoss: 8403927.000000\n",
      "Train Epoch: 6 [2746700/249740 (44%)]\tLoss: 8436466.000000\n",
      "Train Epoch: 6 [2996400/249740 (48%)]\tLoss: 8471366.000000\n",
      "Train Epoch: 6 [3246100/249740 (52%)]\tLoss: 8505214.000000\n",
      "Train Epoch: 6 [3495800/249740 (56%)]\tLoss: 8542882.000000\n",
      "Train Epoch: 6 [3745500/249740 (60%)]\tLoss: 8574378.000000\n",
      "Train Epoch: 6 [3995200/249740 (64%)]\tLoss: 8611761.000000\n",
      "Train Epoch: 6 [4244900/249740 (68%)]\tLoss: 8651705.000000\n",
      "Train Epoch: 6 [4494600/249740 (72%)]\tLoss: 8688835.000000\n",
      "Train Epoch: 6 [4744300/249740 (76%)]\tLoss: 8719627.000000\n",
      "Train Epoch: 6 [4994000/249740 (80%)]\tLoss: 8752177.000000\n",
      "Train Epoch: 6 [5243700/249740 (84%)]\tLoss: 8786283.000000\n",
      "Train Epoch: 6 [5493400/249740 (88%)]\tLoss: 8823513.000000\n",
      "Train Epoch: 6 [5743100/249740 (92%)]\tLoss: 8861059.000000\n",
      "Train Epoch: 6 [5992800/249740 (96%)]\tLoss: 8899327.000000\n",
      "Train Epoch: 6 [6232512/249740 (100%)]\tLoss: 8931678.000000\n",
      "\n",
      "Test set: Accuracy: 78520/83247 (94%)\n",
      "Train Epoch: 7 [0/249740 (0%)]\tLoss: 8932022.000000\n",
      "Train Epoch: 7 [249700/249740 (4%)]\tLoss: 8943051.000000\n",
      "Train Epoch: 7 [499400/249740 (8%)]\tLoss: 8952621.000000\n",
      "Train Epoch: 7 [749100/249740 (12%)]\tLoss: 8965079.000000\n",
      "Train Epoch: 7 [998800/249740 (16%)]\tLoss: 8975843.000000\n",
      "Train Epoch: 7 [1248500/249740 (20%)]\tLoss: 8993033.000000\n",
      "Train Epoch: 7 [1498200/249740 (24%)]\tLoss: 9016054.000000\n",
      "Train Epoch: 7 [1747900/249740 (28%)]\tLoss: 9044557.000000\n",
      "Train Epoch: 7 [1997600/249740 (32%)]\tLoss: 9079751.000000\n",
      "Train Epoch: 7 [2247300/249740 (36%)]\tLoss: 9115530.000000\n",
      "Train Epoch: 7 [2497000/249740 (40%)]\tLoss: 9147765.000000\n",
      "Train Epoch: 7 [2746700/249740 (44%)]\tLoss: 9183889.000000\n",
      "Train Epoch: 7 [2996400/249740 (48%)]\tLoss: 9212998.000000\n",
      "Train Epoch: 7 [3246100/249740 (52%)]\tLoss: 9241650.000000\n",
      "Train Epoch: 7 [3495800/249740 (56%)]\tLoss: 9271126.000000\n",
      "Train Epoch: 7 [3745500/249740 (60%)]\tLoss: 9304250.000000\n",
      "Train Epoch: 7 [3995200/249740 (64%)]\tLoss: 9336088.000000\n",
      "Train Epoch: 7 [4244900/249740 (68%)]\tLoss: 9367611.000000\n",
      "Train Epoch: 7 [4494600/249740 (72%)]\tLoss: 9399975.000000\n",
      "Train Epoch: 7 [4744300/249740 (76%)]\tLoss: 9436735.000000\n",
      "Train Epoch: 7 [4994000/249740 (80%)]\tLoss: 9470340.000000\n",
      "Train Epoch: 7 [5243700/249740 (84%)]\tLoss: 9499866.000000\n",
      "Train Epoch: 7 [5493400/249740 (88%)]\tLoss: 9528777.000000\n",
      "Train Epoch: 7 [5743100/249740 (92%)]\tLoss: 9558936.000000\n",
      "Train Epoch: 7 [5992800/249740 (96%)]\tLoss: 9586036.000000\n",
      "Train Epoch: 7 [6232512/249740 (100%)]\tLoss: 9611775.000000\n",
      "\n",
      "Test set: Accuracy: 78649/83247 (94%)\n",
      "Train Epoch: 8 [0/249740 (0%)]\tLoss: 9612077.000000\n",
      "Train Epoch: 8 [249700/249740 (4%)]\tLoss: 9621313.000000\n",
      "Train Epoch: 8 [499400/249740 (8%)]\tLoss: 9632778.000000\n",
      "Train Epoch: 8 [749100/249740 (12%)]\tLoss: 9642067.000000\n",
      "Train Epoch: 8 [998800/249740 (16%)]\tLoss: 9655890.000000\n",
      "Train Epoch: 8 [1248500/249740 (20%)]\tLoss: 9674328.000000\n",
      "Train Epoch: 8 [1498200/249740 (24%)]\tLoss: 9697760.000000\n",
      "Train Epoch: 8 [1747900/249740 (28%)]\tLoss: 9727434.000000\n",
      "Train Epoch: 8 [1997600/249740 (32%)]\tLoss: 9754730.000000\n",
      "Train Epoch: 8 [2247300/249740 (36%)]\tLoss: 9787537.000000\n",
      "Train Epoch: 8 [2497000/249740 (40%)]\tLoss: 9820718.000000\n",
      "Train Epoch: 8 [2746700/249740 (44%)]\tLoss: 9847639.000000\n",
      "Train Epoch: 8 [2996400/249740 (48%)]\tLoss: 9880547.000000\n",
      "Train Epoch: 8 [3246100/249740 (52%)]\tLoss: 9909553.000000\n",
      "Train Epoch: 8 [3495800/249740 (56%)]\tLoss: 9943609.000000\n",
      "Train Epoch: 8 [3745500/249740 (60%)]\tLoss: 9963968.000000\n",
      "Train Epoch: 8 [3995200/249740 (64%)]\tLoss: 9984637.000000\n",
      "Train Epoch: 8 [4244900/249740 (68%)]\tLoss: 10011457.000000\n",
      "Train Epoch: 8 [4494600/249740 (72%)]\tLoss: 10041199.000000\n",
      "Train Epoch: 8 [4744300/249740 (76%)]\tLoss: 10068028.000000\n",
      "Train Epoch: 8 [4994000/249740 (80%)]\tLoss: 10094744.000000\n",
      "Train Epoch: 8 [5243700/249740 (84%)]\tLoss: 10121941.000000\n",
      "Train Epoch: 8 [5493400/249740 (88%)]\tLoss: 10146498.000000\n",
      "Train Epoch: 8 [5743100/249740 (92%)]\tLoss: 10169347.000000\n",
      "Train Epoch: 8 [5992800/249740 (96%)]\tLoss: 10197913.000000\n",
      "Train Epoch: 8 [6232512/249740 (100%)]\tLoss: 10221040.000000\n",
      "\n",
      "Test set: Accuracy: 78860/83247 (94%)\n",
      "Train Epoch: 9 [0/249740 (0%)]\tLoss: 10221457.000000\n",
      "Train Epoch: 9 [249700/249740 (4%)]\tLoss: 10236536.000000\n",
      "Train Epoch: 9 [499400/249740 (8%)]\tLoss: 10245776.000000\n",
      "Train Epoch: 9 [749100/249740 (12%)]\tLoss: 10255111.000000\n",
      "Train Epoch: 9 [998800/249740 (16%)]\tLoss: 10266932.000000\n",
      "Train Epoch: 9 [1248500/249740 (20%)]\tLoss: 10282603.000000\n",
      "Train Epoch: 9 [1498200/249740 (24%)]\tLoss: 10307615.000000\n",
      "Train Epoch: 9 [1747900/249740 (28%)]\tLoss: 10335835.000000\n",
      "Train Epoch: 9 [1997600/249740 (32%)]\tLoss: 10366155.000000\n",
      "Train Epoch: 9 [2247300/249740 (36%)]\tLoss: 10394078.000000\n",
      "Train Epoch: 9 [2497000/249740 (40%)]\tLoss: 10429034.000000\n",
      "Train Epoch: 9 [2746700/249740 (44%)]\tLoss: 10466569.000000\n",
      "Train Epoch: 9 [2996400/249740 (48%)]\tLoss: 10492133.000000\n",
      "Train Epoch: 9 [3246100/249740 (52%)]\tLoss: 10516055.000000\n",
      "Train Epoch: 9 [3495800/249740 (56%)]\tLoss: 10536758.000000\n",
      "Train Epoch: 9 [3745500/249740 (60%)]\tLoss: 10561625.000000\n",
      "Train Epoch: 9 [3995200/249740 (64%)]\tLoss: 10588051.000000\n",
      "Train Epoch: 9 [4244900/249740 (68%)]\tLoss: 10611628.000000\n",
      "Train Epoch: 9 [4494600/249740 (72%)]\tLoss: 10641328.000000\n",
      "Train Epoch: 9 [4744300/249740 (76%)]\tLoss: 10663810.000000\n",
      "Train Epoch: 9 [4994000/249740 (80%)]\tLoss: 10688694.000000\n",
      "Train Epoch: 9 [5243700/249740 (84%)]\tLoss: 10718320.000000\n",
      "Train Epoch: 9 [5493400/249740 (88%)]\tLoss: 10746275.000000\n",
      "Train Epoch: 9 [5743100/249740 (92%)]\tLoss: 10779449.000000\n",
      "Train Epoch: 9 [5992800/249740 (96%)]\tLoss: 10809473.000000\n",
      "Train Epoch: 9 [6232512/249740 (100%)]\tLoss: 10836317.000000\n",
      "\n",
      "Test set: Accuracy: 78929/83247 (94%)\n",
      "Train Epoch: 10 [0/249740 (0%)]\tLoss: 10836544.000000\n",
      "Train Epoch: 10 [249700/249740 (4%)]\tLoss: 10844891.000000\n",
      "Train Epoch: 10 [499400/249740 (8%)]\tLoss: 10854198.000000\n",
      "Train Epoch: 10 [749100/249740 (12%)]\tLoss: 10868187.000000\n",
      "Train Epoch: 10 [998800/249740 (16%)]\tLoss: 10883526.000000\n",
      "Train Epoch: 10 [1248500/249740 (20%)]\tLoss: 10896173.000000\n",
      "Train Epoch: 10 [1498200/249740 (24%)]\tLoss: 10917199.000000\n",
      "Train Epoch: 10 [1747900/249740 (28%)]\tLoss: 10951454.000000\n",
      "Train Epoch: 10 [1997600/249740 (32%)]\tLoss: 10989467.000000\n",
      "Train Epoch: 10 [2247300/249740 (36%)]\tLoss: 11023380.000000\n",
      "Train Epoch: 10 [2497000/249740 (40%)]\tLoss: 11054208.000000\n",
      "Train Epoch: 10 [2746700/249740 (44%)]\tLoss: 11081059.000000\n",
      "Train Epoch: 10 [2996400/249740 (48%)]\tLoss: 11106447.000000\n",
      "Train Epoch: 10 [3246100/249740 (52%)]\tLoss: 11132418.000000\n",
      "Train Epoch: 10 [3495800/249740 (56%)]\tLoss: 11157529.000000\n",
      "Train Epoch: 10 [3745500/249740 (60%)]\tLoss: 11181725.000000\n",
      "Train Epoch: 10 [3995200/249740 (64%)]\tLoss: 11211682.000000\n",
      "Train Epoch: 10 [4244900/249740 (68%)]\tLoss: 11240739.000000\n",
      "Train Epoch: 10 [4494600/249740 (72%)]\tLoss: 11264099.000000\n",
      "Train Epoch: 10 [4744300/249740 (76%)]\tLoss: 11288952.000000\n",
      "Train Epoch: 10 [4994000/249740 (80%)]\tLoss: 11309308.000000\n",
      "Train Epoch: 10 [5243700/249740 (84%)]\tLoss: 11328840.000000\n",
      "Train Epoch: 10 [5493400/249740 (88%)]\tLoss: 11354480.000000\n",
      "Train Epoch: 10 [5743100/249740 (92%)]\tLoss: 11385554.000000\n",
      "Train Epoch: 10 [5992800/249740 (96%)]\tLoss: 11412826.000000\n",
      "Train Epoch: 10 [6232512/249740 (100%)]\tLoss: 11439974.000000\n",
      "\n",
      "Test set: Accuracy: 79004/83247 (94%)\n"
     ]
    }
   ],
   "source": [
    "model = CNN1().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "err_log = {'test': [], 'train': []}\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    train(i + 1)\n",
    "    test(err_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization gave extra 1-2% of accuracy, dropout did smth weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data augmentation\n",
    "Data augmentation is another way we can reduce overfitting on models, where we increase the amount of training data using information only in our training data. It is common knowledge that the more data an ML algorithm has access to, the more effective it can be. Even when the data is of lower quality, algorithms can actually perform better, as long as useful data can be extracted by the model from the original data set. For example, text-to-speech and text-based models have improved significantly due to the release of a trillion-word corpus by Google. This result is despite the fact that the data is collected from unfiltered web pages and contains many errors. With such large and unstructured data sets, however, the task becomes one of finding structure within a sea of unstructured data.\n",
    "\n",
    "However, alternative approaches exist. Rather than starting with an extremely large corpus of unstructured and unlabeled data, can we instead take a small, curated corpus of structured data and augment in a way that increases the performance of models trained on it? This approach has proven effective in multiple problems.\n",
    "\n",
    "A very generic and accepted current practice for augmenting image data is to perform geometric and color augmentations, such as reflecting the image, cropping and translating the image, and changing the color palette of the image. Specifically, digit data was augmented with elastic deformations, in addition to the typical affine transformation.\n",
    "\n",
    "#### Exercises\n",
    "1. Try to use some simple augmentation techniques, e.g. rotation, scaling and etc.\n",
    "2. Does it help to improve the quality of classification?\n",
    "3. You can read [this paper](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of pixels from 48 to 96 gave slight advantage over previous approach.\n",
    "Rotation and inversion gave nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modern architecture\n",
    "In practice, it is better to use whatever works best on [ImageNet](http://www.image-net.org). If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you will be pleased to know that in 90% or more of applications you should not have to worry about these. Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch.\n",
    "\n",
    "It should be noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google’s inception architectures and also in current (state of the art) residual networks from Microsoft Research Asia. Both of these (see details below) feature more intricate and different connectivity structures.\n",
    "\n",
    "**GoogLeNet** The ILSVRC 2014 winner was a Convolutional Network from Google. Its main contribution was the development of an [inception module](https://arxiv.org/pdf/1409.4842) that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses average pooling instead of fully connected layers at the top of the net, eliminating a large amount of parameters that do not seem to matter much. There are also several versions to the GoogLeNet, most recently [inception-v4](https://arxiv.org/pdf/1602.07261).\n",
    "\n",
    "**ResNet** [Residual networks](https://arxiv.org/pdf/1512.03385) developed was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. ResNets are currently by far state of the art conv net models and are the default choice for using in practice. In particular, also see more recent developments that tweak the original architecture, e.g. in [this paper](https://arxiv.org/pdf/1603.05027).\n",
    "\n",
    "#### Exercises\n",
    "1. Try to adopt modern architecture for your task.\n",
    "2. Please, explain your decision. What problems have you encountered?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
